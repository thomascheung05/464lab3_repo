{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c00b10-5edd-45d0-83ab-83693c41f056",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of this lab is to familiarize yourself with standard data wrangling tasks using Python. Being able to do so will greatly facilitate any work you do from here onward. After this lab, you should be comfortable enough in a Python data environment that we can begin using a variety of packages to analyze and visualize different kinds of data more freely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703724bc-177c-4915-bc7e-e3e905d5c039",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Your second (and last) week as a data specialist at Via Rail\n",
    "\n",
    "Your supervisor at Via Rail has handed over more 'information'. This time, it's a few poorly formatted tables stored as *.txt* file. They want a basic map, and since Via is *finally* trying to enter the digital age with all this new federal funding, they want this map to create itself with the click of a button (in this case, by running all code cells in this notebook). Like last week, you assume there will be more data coming your way in this format, so you will need to come up with an automated process that imports it, parses it, and does any subesquent operations that are necessary in order to finally prep it for the mapping intern. In other words, you need to write a program that takes data of a certain format as input, and generates clean data as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3afb7b-6828-495e-acb9-68d98fd16659",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "\n",
    "At the start of any Python script (or Jupyter notebook running Python) you will usually want to import packages so that their functionality is available to the rest of your code. One of Python's great strengths is the quality of its 3rd party packages, which can provide functionality for doing anything from video editing to running a web server to... managing data! In this session, we would like to import the [Pandas](https://pandas.pydata.org/) package. Pandas allows us to manipulate data stored in Pandas dataframes, and forms the bedrock of most standard data analysis done with Python today. Familiarity with other basic data management software (e.g. Excel) will serve you well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f5984-e861-4381-b6dc-60b3e20ef2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below command imports pandas\n",
    "import pandas\n",
    "\n",
    "#when importing a package, you can give it the alias of your choice by appending 'as' followed by your alias.\n",
    "#doing so can make it more practical for you to refer to the package in your code, especially if the package were to have a really long name...\n",
    "import pandas as pd\n",
    "\n",
    "#from here on, we can refer to pandas simply as 'pd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf32a8-5361-4d65-ba3c-312e74424cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspecting and importing data\n",
    "\n",
    "Make sure you *know* your data. The first thing you should always do before importing data is to inspect it.\n",
    "- From your command line or file explorer (Windows Explorer/Finder), open *stations.txt* in a text editor (Notepad/Text Edit)\n",
    "\n",
    "What do you observe? Look for patterns in the data: can you spot any characteristics that you could use to parse it into a table? Ideally, you would like to convert this into a clean table where each data point is separated into its own cell under a header. If you are good in Excel, you could easily clean this up in that software. However, you want your process to be as automated as possible, so your initial script will involve some of what you did last week.\n",
    "\n",
    "1. What would you say is the 'delimiter' or 'separator' for this data? Doesthe data have headers? Indicate your answers in a Markdown cell below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec7c02b-f5ce-455c-9324-4b86c9bc206f",
   "metadata": {},
   "source": [
    "Each entry is delimited with semi-colons (;), which would correspond to different variables separated into columns in a dataframe. However, these variables/columns do not have names/headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2540b-16b8-4827-b3f6-650a418229f2",
   "metadata": {},
   "source": [
    "- close the file *stations.txt*.\n",
    "\n",
    "Now you're going to import this dataset. Pandas has a method called [*read_csv*](https://Pandas.pydata.org/docs/user_guide/io.html#csv-text-files), which can be used to import almost any kind of table into a [Pandas dataframe](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe). Although your data doesn't have headers, there is a file in this lab which lists these...\n",
    "\n",
    "The *read_csv* Pandas method can take a series of optional parameters, which we call 'arguments' and 'keyword arguments' respectively. **To continue, you will need to indicate a value for each of the two following variables.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28892151-63bf-4d85-8435-22b595770b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will begin by filling a pathname so the read_csv method can find the stations.txt file\n",
    "#you may need to modify this pathname to reflect the location of the stations.txt file on your computer\n",
    "path_stations = \"stations.txt\"\n",
    "\n",
    "#a string (my_sep) will be passed to the 'sep' argument of the read_csv method\n",
    "#and the other, a list of strings (my_headers), will be used by the 'names' argument. You can find these headers in headers.txt\n",
    "my_sep = \";\"\n",
    "my_headers = #copy this in from the headers.txt file\n",
    "my_headers = my_headers.split(my_sep)\n",
    "\n",
    "df_stations = pd.read_csv(path_stations, sep = my_sep, header = None, names = my_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd5276-7ccf-4153-9bcc-29e8c21079da",
   "metadata": {},
   "source": [
    "Yay! Now you can start working with Pandas.\n",
    "\n",
    "You now have a Pandas dataframe stored in a variable called *df_stations*.\n",
    "The print statement can be used to display the dataframe once imported, but Jupyter is able to render dataframes in a more appealing way. Find active variables in the side panes of Google Colab, or, in VS Code, under the 'Jupyter' tab of the bottom tray (beside the terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46afbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52519a0-f594-4eed-b9db-4e4503177aca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring Pandas dataframes\n",
    "\n",
    "You can see that not all your data is visible. It would take up a lot of space to display all your data like an Excel sheet (besides, you have other software for doing that). You can use the following dataframe methods to explore your data in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16724b-e961-41a3-9bae-7db943be764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#head will show you the top 5 rows\n",
    "df_stations.head()\n",
    "\n",
    "#you can also add a number indicating how many rows you would like to see inside the parentheses ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca89798-d423-461c-9279-cf530cde5b68",
   "metadata": {},
   "source": [
    "You can also use the tail method to show the last rows of a dataframe.\n",
    "\n",
    "2. Use the tail method with the appropriate argument in the code cell below to show the last 15 rows of *stations_df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82f4b7-d01b-434d-9c9c-54a9d4c1f8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98aba72-cec4-4deb-a2e8-b762f59c0823",
   "metadata": {},
   "source": [
    "Below are some other commands you can use to explore your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48043d8-e29b-424b-bf01-058764e80ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape will return a tuple containing the number of rows followed by the number of columns.\n",
    "df_stations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c13679-494c-46cc-b1b6-b769792d3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info will show you useful information related to your data\n",
    "df_stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02491fa-6950-433f-a237-1f0c74d27ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe will compute some summary statistics about your data\n",
    "#note that such statistics may be meaningless depending on the nature of your data\n",
    "df_stations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf0489-56a9-4f67-bc17-4a3233f2bccf",
   "metadata": {},
   "source": [
    "3. Based on the information provided by the commands above, how many stations are there in your data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "885f4991-40d4-4b1f-b312-3fbdf1d16db9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63aa8078-0289-45ce-b9b5-ca33e96e83c9",
   "metadata": {},
   "source": [
    "4. Based on the information provided by the commands above, how many different cities are represented in your data? You may wish to run another command to better elucidate this info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3c035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5cc00b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94f6470-acc8-4ae1-b6fd-00f6d583db7f",
   "metadata": {},
   "source": [
    "5. Why do you think the *describe()* method is only providing summary stats for a limited number of fields?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de4ea326-cd5d-4a2d-9684-888e25529850",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71463b21-d9d3-4275-9328-a4e8b7fc009a",
   "metadata": {},
   "source": [
    "You can easily create new dataframes by subsetting; or, in other words, selecting specific columns to assign to a new dataframe. This is great for shedding columns you might not need. Subsetting a dataframe has similarities to how you would do it with a dictionary: You use the [] to select fields by their header names. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7443515-cd86-4d82-82fb-3d3c75113ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to isolate a single column, simply select the header like you would a key in a dictionary.\n",
    "#This creates a Pandas 'series', which is kind of analogous to a list. It's basically a one-dimensional dataframe...\n",
    "station_names = df_stations['stat_name']\n",
    "station_names.head()\n",
    "\n",
    "#pass station_names to the type() function. What data type is it?\n",
    "type(station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad745d7-ba12-45e0-b8ad-8fe6bd31cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example, let's create a new dataframe that only contains the station ID and station name.\n",
    "#to subset multiple columns to a new dataframe, you will need to pass a list of these headers\n",
    "little_df = df_stations[['stat_ID','stat_name']]\n",
    "little_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e97ed-720b-43d6-b977-933657bc33fc",
   "metadata": {},
   "source": [
    "6. In the code cell below, create a new dataframe called *temp_df* that only contains the following fields: *stat_ID*, *stat_name*, *city*, *postal_code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131c9e2-da21-4ad6-9e72-51230457902c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f891c3a-a1c9-42e5-9c33-6bc2ff3c6435",
   "metadata": {},
   "source": [
    "Another way of shedding unneeded columns is simply to delete them in-place (within the original dataframe). In *df_stations*, There are columns which you could do without. These would be the following: *object_ID*, *address_2*\n",
    "\n",
    "These two fields are by most measures useless. You can tell because they are either zombie ID fields, or contain so few values that their use in any scenario is questionable. Search for the delete method in the [Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html) and delete these fields!\n",
    "\n",
    "(hint: you might want to check inside the page on dataframes...)\n",
    "\n",
    "Once you find the relevant information, you will see that there are two common ways of deleting fields: 'del', and the more familiar 'pop' method used with lists.\n",
    "\n",
    "7. Use the pop method to delete these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094b3d4-4468-4f91-9940-60a9797820d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pop method\n",
    "\n",
    "#then run the cell to display your dataframe below and make sure everything is as expected\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b40ace-13db-460b-8760-61addac8848b",
   "metadata": {},
   "source": [
    "8. Once you have run your deletions in the cell above, run the code cell again. Why does it fail? What is a KeyError?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eaec642-6d24-4da7-9873-c3e8a682d1e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15e98229-2a2e-48fa-b117-4af6e63d5096",
   "metadata": {},
   "source": [
    "## Preparing your data for the mapping intern\n",
    "\n",
    "Now that your table is all cleaned up, it's time to prepare it for mapping. There's one issue though: the data you've been working with so far doesn't seem to have any explicit location information (i.e. coordinates). There is other geographic information in your table, however, which you could potentially exploit for mapping purposes. Can you name which fields these would be?\n",
    "\n",
    "9. Which fields do you think you would need in order to [geocode](https://desktop.arcgis.com/en/arcmap/latest/manage-data/geocoding/what-is-geocoding.htm) these transit stations? Name all that apply in the Markdown cell below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42e4bcdc-4784-4b40-b912-3b86ee3e8cf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc5c790d-1525-4f04-b00d-dc5563354dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Luckily, you won't need to be doing any geocoding today, since you noticed that your supervisor also left you with another file called *coordinates.txt*.\n",
    "\n",
    "10. Open the file in a text editor and inspect it like you did with the stations file. Do you notice a pattern here which you could use for parsing the data?\n",
    "- What would the delimiter be for this data? What file extension do we typically use to associate to this delimiter?\n",
    "- Does the data have headers?\n",
    "- Can you identify a field in *coordinates.txt* that you could use to merge with your stations data? Which field?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557e6cb6-8f21-4abb-bb92-717f4cd5de38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c31652-d24f-4ca1-91ed-85de1313076d",
   "metadata": {},
   "source": [
    "11. Import *coords.txt* into JupyterLab using the appropriate Pandas method (you've done this before!) and assign the dataframe to a variable called *df_coords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2038585-c2ee-43af-ae83-a4d0c30ccea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dffa57a-831c-43c8-8288-b18657d25127",
   "metadata": {},
   "source": [
    "## Merging dataframes\n",
    "\n",
    "You will need to join your coordinate data with your station information. You can do this using the Pandas [merge method](https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92234679-2e73-46f4-bc48-dc804bc4f41f",
   "metadata": {},
   "source": [
    "Before doing a merge, you should identify which column can be used to perform the merge. [Merges](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) are very similar to standard [join operations](https://dataschool.com/assets/images/how-to-teach-people-sql/sqlJoins/sqlJoins_7.png) in SQL, which you have probably seen in software like ArcGIS before (e.g. joining some data to the attribute table of a shapefile based on a common identifier).\n",
    "\n",
    "To merge tables, you will need to identify a column that contains values which can be used to match rows from each table to eachother.\n",
    "\n",
    "**Take note of which column you could use as a join field for merging *df_stations* with *df_coords*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a7d09-9f4b-42f3-8480-18f39829a668",
   "metadata": {},
   "source": [
    "Before performing a merge, you want to make sure there are no duplicates in your data. A duplicate could mean that the same row from the table being merged gets joined twice or more. The [duplicated method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) can serve you well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2834276-f8bf-4b75-abde-5660f3062bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_coords.duplicated()\n",
    "\n",
    "#below is a little loop for verifying if there are any duplicates (note that there are simpler ways of doing this with Pandas than using a for loop)\n",
    "for x in duplicates:\n",
    "    if x == True:\n",
    "        print(x)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e5fe0-93cc-46ac-8093-366ab3d6957f",
   "metadata": {},
   "source": [
    "12. What data type is being generated by the duplicated() method? Are there any duplicates in *df_stations*? Run a code cell to determine this, then a short text explanation in a Markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2809e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2034db-b8e7-4074-9c91-0b060aadb669",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c632597-1106-4e5a-b570-f159d70ac6e2",
   "metadata": {},
   "source": [
    "The code below will test whether there are any duplicates specifically within the *stat_ID* field, which could be a good field for eventually merging the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966487d-9c87-434e-9c62-68128a62ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_stations['stat_ID'].duplicated()\n",
    "\n",
    "for x in duplicates:\n",
    "    if x == True:\n",
    "        print(x)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb2e46-d91c-4d02-98e4-cc845c51d748",
   "metadata": {},
   "source": [
    "### Using a custom function to check for duplicates\n",
    "\n",
    "[Functions](https://www.w3schools.com/python/python_functions.asp) are a great way to organize your code. Basically, you can wrap any code inside a function, which allows you to call that code from anywhere else in your code without having to rewrite the code all over again! Think of it as a more elaborate variable, but a variable that *does* stuff, instead of just holding static information. Like variables, you need to declare a function first before using it. After that, you can refer to it anywhere, as long as you pass it the right arguments!\n",
    "\n",
    "Functions in Python are declared using the 'def' keyword. Immediately following the name of the function - which is up to you to choose - and much like any variable name, you need to add parentheses (()). Inside these parentheses is where you can pass arguments to the function (i.e. input data).\n",
    "\n",
    "Like conditional statements or loops, function declarations are immediately followed by a colon (:), and any code nested inside a function is indented. The 'return' statement at the end of a function signals what it will output. Think of the () of the function as the point of entry, and the 'return' statement as the exit.\n",
    "\n",
    "13. You would like to write a custom function to check for duplicates so that you can freely call it from anywhere in your code. Your function should require a **list of booleans as input**, and **return a number indicating the number of duplicates** (i.e. True values) in the list that was passed to it. Use the documentation provided above to familiarize yourself with functions (and any other documentation you can find).\n",
    "\n",
    "Hint: len() will provide the length of a list (and therefore a number). .append() can be used for appending True values to a list, which you can then use len() on as your return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20115843-7967-4c31-8c80-cb8de92b5053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b974ea80-d26a-47f5-a6aa-438d1a08e79e",
   "metadata": {},
   "source": [
    "14. How many duplicates are there in the *city* field of df_stations? Produce the result in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2c799-4ea5-41b3-a663-65221d3807b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc074f5-4d56-4148-804c-8c111aadb8f1",
   "metadata": {},
   "source": [
    "### Merging at last!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a3a32-ab95-4e74-a10b-f52d91697937",
   "metadata": {},
   "source": [
    "Now that you are certain there are no duplicates, you can feel confident about merging these tables. \n",
    "\n",
    "15. Finally, use the documentation (links) provided above (under the **Merging dataframes** title) to perform this merge. You can also use Google or ask an AI chatbot if you need other resources. You will need to do your own research here. However, the task shouldn't be too complex, and you shouldn't have to use too many of the optional parameters (keyword arguments) that the merge method provides. Assign your merged data to a dataframe called *df_stations_coords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c199556-fe55-465a-86d3-91b52125d89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29ea8dab-93ac-4139-a872-bb878b276124",
   "metadata": {},
   "source": [
    "Once you are finished, from the toolbar at the top of JupyterLab/Colab, click Kernel > **Restart Kernel and Clear All Outputs...**. Once the notebook has completed rebooting, click Run > **Run All Cells**. Make sure that all your code runs properly, and produces your merged table at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27f2b6-f6aa-4628-ba14-6252924f6d9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ebda3-a4e1-48a9-9f1e-a09fd74ca780",
   "metadata": {},
   "source": [
    "Fed up with being an intermediary between your supervisor's lousy data and the pesky mapping interns, you decide to take on greater challenges. You can do the mapping, AND you can find quality data for doing so. Further, you have ambitions: you want to explore more interesting data that you can analyze and do research with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5eddf4-c8a9-4091-87ee-7f24345ec22d",
   "metadata": {},
   "source": [
    "# Deliverables\n",
    "\n",
    "Complete the questions in this notebook and make sure all your code runs, then upload the notebook file (*.ipynb*) to the Assignment page on Moodle. If you ran this lab in Colab, you will need to export/download the notebook file in order to upload it.\n",
    "\n",
    "Optionally, you can also create another GitHub repository for this lab and push/commit this lab file to it. For the upcoming labs, no use of GitHub repos will be necessary for marks, but they will come in handy towards the end of the term for the hosting of your webmap project, so familiarising yourself with GitHub may not be the worst idea..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
